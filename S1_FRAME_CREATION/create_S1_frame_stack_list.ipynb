{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify files in Sentinel-1 archive to create 'frame' scene stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires 'run_S1_bursts' in GAMMA repo (S1_FRAME_CREATION dir) to be run first to generate up to date dataframes for each track. These dataframes contain the metadata and burst information for each zip file in the archive, and are used as input for this notebook.\n",
    "\n",
    "If adding scenes to existing frame list, use 'add_S1_frame_stack_list.ipynb' instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# developed 13 Mar 2019, S Lawrie, GA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import datetime\n",
    "import fiona\n",
    "\n",
    "burst_dir = \"/g/data1/dg9/SENTINEL-1_BURSTS\"\n",
    "\n",
    "%matplotlib inline \n",
    "# required for correct plotting in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Track and frame details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PROJECT DETAILS\n",
    "\n",
    "# project name used in INSAR_ANALYSIS directory\n",
    "project = \"xxxx\"\n",
    "\n",
    "# track and frame details from master frame list (numbers only)\n",
    "track = xxx\n",
    "frame = xx\n",
    "\n",
    "# orientation \"Ascending\" or \"Descending\"\n",
    "orient = \"xxxx\"\n",
    "\n",
    "# polarisation \"VV\" or \"HH\"\n",
    "polar = \"xxxx\n",
    "\n",
    "# time period to find matching scenes \n",
    "start_period = 20140101\n",
    "end_period = 201xxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up project processing directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create project processing directory\n",
    "insar_dir = \"/g/data1/dg9/INSAR_ANALYSIS\"\n",
    "project_dir = os.path.join(insar_dir,project,\"S1\",\"GAMMA\")\n",
    "kml_dir = os.path.join(project_dir,\"kmls\")\n",
    "\n",
    "# make directories\n",
    "if not os.path.exists(project_dir):\n",
    "    os.makedirs(project_dir)\n",
    "if not os.path.exists(kml_dir):\n",
    "    os.makedirs(kml_dir)\n",
    "    \n",
    "if orient == \"Ascending\":\n",
    "    orient2 = \"A\"\n",
    "elif orient == \"Descending\":\n",
    "    orient2 = \"D\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to identify matching scenes, create KML file and produce text file for input into GAMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class id_stack_scenes:\n",
    "    def __init__(self,burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period):\n",
    "        self.burst_dir = burst_dir\n",
    "        self.project_dir = project_dir\n",
    "        self.kml_dir = kml_dir\n",
    "        self.orient = orient\n",
    "        self.orient2 = orient2\n",
    "        self.track = track\n",
    "        self.frame = frame\n",
    "        self.polar = polar\n",
    "        self.start = start_period\n",
    "        self.end = end_period\n",
    "        \n",
    "    def subset_archive(self):\n",
    "        \"Load saved dataframes and subset them by orientation, polarisation, track and frame.\" \n",
    "        \"Find which archive bursts fit within the master burst buffer coordinates.\"\n",
    "        \n",
    "        # load saved master burst coordinates dataframe\n",
    "        input_master = '%s/MASTER_BURSTS/S1_IW_SLC_%s_Master_Bursts_Dataframe' %(self.burst_dir,self.orient)\n",
    "        self.df_master = pd.read_pickle(input_master)\n",
    "        \n",
    "        # load saved archive burst coordinates dataframe\n",
    "        input_archive = '%s/ARCHIVE_TRACKS/%s_track_data/S1_IW_SLC_T%s%s_Dataframe' %(self.burst_dir,self.orient,self.track,self.orient2)\n",
    "        self.df_archive = pd.read_pickle(input_archive)\n",
    "        \n",
    "        # subset master burst coordinates to desired scene stack\n",
    "        self.df_master_subset = self.df_master.loc[(self.df_master['Pass'] == self.orient) & \n",
    "                                                   (self.df_master['RelOrbit'] == self.track) & \n",
    "                                                   (self.df_master['Frame'] == self.frame) & \n",
    "                                                   (self.df_master['Polar'].str.contains(self.polar))]\n",
    "\n",
    "        # create KML file of master frame\n",
    "        master_merged = self.df_master_subset.dissolve(by='Mode') # merge all polygons into one\n",
    "        master_merged.reset_index(level=0, inplace=True) \n",
    "        master_merged2 = master_merged.drop(columns=['Buffer']) # remove buffer column to enable correct coordinates to be used\n",
    "        fiona.supported_drivers['KML'] = 'rw'\n",
    "        out_kml = '%s/S1_T%s%s_F%s_%s.kml' %(self.kml_dir,self.track,self.orient2,self.frame,self.polar)\n",
    "        master_merged2.to_file(out_kml, driver='KML')\n",
    "        \n",
    "        # subset archive burst coordinates to desired track by polarisation and date range\n",
    "        df_subset = self.df_archive.loc[(self.df_archive['Polar'] == self.polar)]\n",
    "        self.df_archive_subset = df_subset[df_subset['Date'].between(start_period, end_period, inclusive=True)]\n",
    "\n",
    "        # create blank dataframe to save matching bursts in archive\n",
    "        self.df_archive_matching = pd.DataFrame([]) \n",
    "        \n",
    "        # select archive bursts that fall within master bursts\n",
    "        swaths = self.df_master_subset.Swath.unique()\n",
    "        self.swath_list = swaths.tolist()\n",
    "    \n",
    "        for x in self.swath_list:\n",
    "            swath_bursts = self.df_master_subset.loc[self.df_master_subset['Swath'] == x]\n",
    "            bursts = swath_bursts['BurstNum']\n",
    "            burst_list = bursts.tolist()  \n",
    "    \n",
    "            for y in burst_list:\n",
    "                burst_row = swath_bursts.loc[swath_bursts['BurstNum'] == y].reset_index(drop=True)\n",
    "                buffer = burst_row['Buffer'][0]\n",
    "                frame = burst_row['Frame'][0]\n",
    "                mas_burst_num = burst_row['BurstNum'][0]\n",
    "                mas_swath = burst_row['Swath'][0]\n",
    "\n",
    "                # iterate over archive and append matching bursts to blank dataframe\n",
    "                for i, row in self.df_archive_subset.iterrows():\n",
    "                    poly = row['Extent']\n",
    "                    check = buffer.contains(poly)\n",
    "                    if check == True:\n",
    "                        date = row[2]\n",
    "                        pass1 = row[3]\n",
    "                        polar = row[4]\n",
    "                        rel_orbit = row[5]\n",
    "                        ipf = row[6]\n",
    "                        raw_date = row[7]\n",
    "                        org_burst_num = row[8]\n",
    "                        org_swath = row[9]\n",
    "                        grid = row[11]\n",
    "                        zip_file = row[12]\n",
    "                        org_max_burst = row[13]  \n",
    "                        df_temp = gpd.GeoDataFrame([[date,pass1,polar,rel_orbit,frame,ipf,raw_date,org_swath,\n",
    "                                                     org_burst_num,org_max_burst,mas_swath,mas_burst_num,poly,\n",
    "                                                     grid,zip_file]],\n",
    "                                                   columns = ['Date','Pass','Polar','RelOrbit','Frame','IPFVer',\n",
    "                                                              'RawDate','OrgSwath','OrgBurstNum','OrgMaxBurst',\n",
    "                                                              'MasSwath','MasBurstNum','Extent','GridDir','ZipFile'],\n",
    "                                                   geometry='Extent')                    \n",
    "                        self.df_archive_matching = self.df_archive_matching.append(df_temp, ignore_index=True)    \n",
    "\n",
    "        return self.df_archive_matching\n",
    "        \n",
    "    def dem_extent(self):        \n",
    "        \"Use master burst coordinates to determine DEM extent for auto DEM generation in GAMMA.\"       \n",
    "        \n",
    "        # merge burst polygons to single polygon (using burst coords)\n",
    "        buffer_coords = self.df_master_subset[['Mission','Buffer']]\n",
    "        polys1 = gpd.GeoDataFrame(buffer_coords,geometry='Buffer') \n",
    "        polys2 = polys1.dissolve(by='Mission')\n",
    "        minx = polys2.bounds.at['S1','minx']\n",
    "        miny = polys2.bounds.at['S1','miny']\n",
    "        maxx = polys2.bounds.at['S1','maxx']\n",
    "        maxy = polys2.bounds.at['S1','maxy']\n",
    "\n",
    "        # create new dataframe with overall frame coords\n",
    "        ul = Point(minx,maxy)\n",
    "        ur = Point(maxx,maxy)\n",
    "        lr = Point(maxx,miny)\n",
    "        ll = Point(minx,miny)\n",
    "        pointList = [ul, ur, lr, ll]\n",
    "        pol2 = Polygon([[p.x, p.y] for p in pointList]) # creates shapely polygon\n",
    "        frame_extent = gpd.GeoDataFrame([[pol2]],columns = ['Extent'],geometry='Extent')\n",
    "\n",
    "        # add additional buffer to frame extent\n",
    "        poly = frame_extent.at[0,'Extent']\n",
    "        frame_buffer = poly.buffer(+0.3, cap_style=2,join_style=2)\n",
    "        frame_extent.at[0,'Buffer'] = frame_buffer\n",
    "        frame2 =  gpd.GeoDataFrame(frame_extent,geometry='Buffer')\n",
    "\n",
    "        # extract coords for text file\n",
    "        fr_minx = frame_extent.bounds.at[0,'minx']\n",
    "        fr_miny = frame_extent.bounds.at[0,'miny']\n",
    "        fr_maxx = frame_extent.bounds.at[0,'maxx']\n",
    "        fr_maxy = frame_extent.bounds.at[0,'maxy']\n",
    "        self.dem_coords = fr_minx,fr_miny,fr_maxx,fr_maxy\n",
    "        \n",
    "        return self.dem_coords    \n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"Identifies and removes duplicates.\"  \n",
    "\n",
    "        # add temporary index column for this step\n",
    "        self.df_archive_matching['idx'] = self.df_archive_matching.index\n",
    "\n",
    "        scene_date_dups = self.df_archive_matching[self.df_archive_matching.duplicated(subset=['Date','MasSwath',\n",
    "                                                                                               'MasBurstNum'],\n",
    "                                                                                       keep=False)]\n",
    "        # 1: identify all unique dates (i.e. no duplicates)\n",
    "        dup_scene_date_index = scene_date_dups.index.tolist() \n",
    "        df_uniq = self.df_archive_matching\n",
    "        df_uniq.drop(df_uniq.index[dup_scene_date_index], inplace = True)\n",
    "\n",
    "        # 2: identify remaining duplicates that have same date but diff IPF version\n",
    "        ipf_max = scene_date_dups.groupby('Date').IPFVer.transform(max)\n",
    "        ipf_dups = scene_date_dups.loc[scene_date_dups.IPFVer == ipf_max]\n",
    "        ipf_uniq = ipf_dups.drop_duplicates(subset=['Date','IPFVer','MasSwath','MasBurstNum'], keep=False)\n",
    "\n",
    "        # 3: identify remaining duplicates that have same date and IPF version but diff raw date\n",
    "        ipf_uniq_index = ipf_uniq.index.tolist()\n",
    "        raw_date_dups1 = ipf_dups[~ipf_dups['idx'].isin(ipf_uniq_index)]\n",
    "        raw_date_max = raw_date_dups1.groupby('Date').RawDate.transform(max)\n",
    "        raw_date_dups = raw_date_dups1.loc[raw_date_dups1.RawDate == raw_date_max]\n",
    "        raw_date_uniq = raw_date_dups.drop_duplicates(subset=['Date','RawDate','MasSwath','MasBurstNum'],\n",
    "                                                      keep=False)\n",
    "\n",
    "        # 4: remove remaining duplcates (i.e. all details the same, keep one row)\n",
    "        raw_date_uniq_index = raw_date_uniq.index.tolist()\n",
    "        remain_date_dups1 = raw_date_dups[~raw_date_dups['idx'].isin(raw_date_uniq_index)]\n",
    "        remain_date_uniq = remain_date_dups1.drop_duplicates(subset = ['Date','IPFVer','RawDate','MasSwath',\n",
    "                                                                       'MasBurstNum'],keep='first')\n",
    "\n",
    "        # 5: join all unique dataframes together to get final matching archive dataframe\n",
    "        df_no_dups1 = pd.concat([df_uniq,ipf_uniq,raw_date_uniq,remain_date_uniq],axis=0,sort=False).reset_index(drop=True)\n",
    "        df_no_dups = df_no_dups1.drop(columns='idx',axis=1)\n",
    "       \n",
    "        # 6: sort final dataframe\n",
    "        self.df_archive_matching_no_dups = df_no_dups.sort_values(['Date','MasSwath','MasBurstNum'],\n",
    "                                                                  ascending=[True,True,True])  \n",
    "     \n",
    "        return self.df_archive_matching_no_dups\n",
    "        \n",
    "    def first_full_scene(self):      ##### WILL NEED MODIFYING TO DEAL WITH ALL SCENES WHICH DON'T HAVE 12 BURSTS\n",
    "        \"Identify first scene which has 12 master bursts (use for resizing any scenes which don't have 12 bursts)\"\n",
    "    \n",
    "        dates = self.df_archive_matching_no_dups.Date.unique()\n",
    "        self.dates_list = dates.tolist()\n",
    "        df_dates = pd.DataFrame([]) \n",
    "    \n",
    "        for x in self.dates_list:\n",
    "            iw1_b1 = self.df_archive_matching_no_dups.loc[(self.df_archive_matching_no_dups['Date'] == x) &\n",
    "                                                          (self.df_archive_matching_no_dups['MasSwath'] == 'IW1') & \n",
    "                                                          (self.df_archive_matching_no_dups['MasBurstNum'] == 1)]\n",
    "            iw1_b12 = self.df_archive_matching_no_dups.loc[(self.df_archive_matching_no_dups['Date'] == x) & \n",
    "                                                           (self.df_archive_matching_no_dups['MasSwath'] == 'IW1') & \n",
    "                                                           (self.df_archive_matching_no_dups['MasBurstNum'] == 12)]  \n",
    "\n",
    "            # check if results are returned for both bursts 1 and 12, if append dates to new dataframe\n",
    "            if iw1_b1.empty or iw1_b12.empty:\n",
    "                pass\n",
    "            else:\n",
    "                df_dates = df_dates.append(iw1_b1,ignore_index=True)\n",
    "                df_dates = df_dates.append(iw1_b12,ignore_index=True)\n",
    "        df1 = df_dates.sort_values(by=['Date'])\n",
    "        self.first_date = df1['Date'].iloc[0]\n",
    "\n",
    "    def org_mas_bursts(self):\n",
    "        \"Summarise the original bursts and the equivalant master burst numbers\"\n",
    "\n",
    "        self.org_mas_bursts = pd.DataFrame([]) \n",
    "\n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_archive_matching_no_dups.loc[self.df_archive_matching_no_dups['Date'] == w]  \n",
    "            for x in self.swath_list:\n",
    "                swath_rows = date_rows.loc[date_rows['MasSwath'] == x]\n",
    "                zip_files = swath_rows.ZipFile.unique()\n",
    "                zip_list = zip_files.tolist() \n",
    "                for z in zip_list:\n",
    "                    rows = swath_rows.loc[swath_rows['ZipFile'] == z]     \n",
    "                    grid = rows.iloc[0,13]      \n",
    "                    min_org_burst = rows.OrgBurstNum.min()\n",
    "                    max_org_burst = rows.OrgBurstNum.max()\n",
    "                    if min_org_burst == max_org_burst:\n",
    "                        org_burst = ('%s' % min_org_burst)\n",
    "                    else:\n",
    "                        org_burst = ('%s-%s') %(min_org_burst,max_org_burst)\n",
    "                    min_mas_burst = rows.MasBurstNum.min()\n",
    "                    max_mas_burst = rows.MasBurstNum.max()\n",
    "                    if min_mas_burst == max_mas_burst:\n",
    "                        mas_burst = ('%s' % min_mas_burst)\n",
    "                    else:\n",
    "                        mas_burst = ('%s-%s') %(min_mas_burst,max_mas_burst)\n",
    "    \n",
    "                    df1 = pd.DataFrame([[w,x,org_burst,mas_burst,grid,z]],columns = ['Date','Swath','OrgBurst',\n",
    "                                                                                     'MasBurst','GridDir','ZipFile'])\n",
    "                    self.org_mas_bursts = self.org_mas_bursts.append(df1, ignore_index=True, sort=False)              \n",
    "\n",
    "        return self.org_mas_bursts\n",
    "            \n",
    "    def max_bursts_swath(self):  \n",
    "        \"Determine the maximum bursts per swath for each zip file\"\n",
    "        \n",
    "        df_max_bursts_swath1 = pd.DataFrame([]) \n",
    "        \n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_archive_matching_no_dups.loc[self.df_archive_matching_no_dups['Date'] == w]     \n",
    "            date_zip_files = date_rows.ZipFile.unique()\n",
    "            date_zip_list = date_zip_files.tolist()\n",
    "\n",
    "            for x in self.swath_list:\n",
    "                swath_zip = self.df_archive.loc[self.df_archive['Swath'] == x]\n",
    "                zip_files = swath_zip.ZipFile.unique()\n",
    "                zip_list = zip_files.tolist() \n",
    "\n",
    "                for y in date_zip_list:\n",
    "                    for z in zip_list:\n",
    "                        if y == z:\n",
    "                            rows = swath_zip.loc[swath_zip['ZipFile'] == y]\n",
    "                            grid = rows.iloc[0,11]\n",
    "                            max_burst = rows['MaxBurst'].max()\n",
    "                            swath = (\"MaxBurst%s\" % x)\n",
    "                            df1 = pd.DataFrame([[w, grid, z, max_burst]],columns = ['Date','GridDir','ZipFile', swath])\n",
    "                            df_max_bursts_swath1 = df_max_bursts_swath1.append(df1,ignore_index=True,sort=False)     \n",
    "\n",
    "            self.df_max_bursts_swath = df_max_bursts_swath1.groupby(['Date','GridDir',\n",
    "                                                                         'ZipFile'])[['MaxBurstIW1',\n",
    "                                                                                      'MaxBurstIW2',\n",
    "                                                                                      'MaxBurstIW3'\n",
    "                                                                                     ]].first().reset_index()   \n",
    "        return self.df_max_bursts_swath\n",
    "            \n",
    "    def download_files(self):\n",
    "        \"Create list of files to download\"\n",
    "        \n",
    "        self.download_files = self.df_max_bursts_swath[['Date','GridDir','ZipFile']]\n",
    "           \n",
    "    def tot_bursts(self):\n",
    "        \"Determine total number of bursts by swath for each date\"\n",
    "\n",
    "        self.df_dates_total = pd.DataFrame([]) \n",
    "    \n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_max_bursts_swath.loc[self.df_max_bursts_swath['Date'] == w]\n",
    "            IW1_sum = date_rows['MaxBurstIW1'].sum()\n",
    "            IW2_sum = date_rows['MaxBurstIW2'].sum()\n",
    "            IW3_sum = date_rows['MaxBurstIW3'].sum()\n",
    "                                   \n",
    "            df1 = pd.DataFrame([[w, IW1_sum, IW2_sum, IW3_sum]],columns = ['Date','TotBurstsIW1','TotBurstsIW2',\n",
    "                                                                           'TotBurstsIW3'])\n",
    "            self.df_dates_total = self.df_dates_total.append(df1, ignore_index=True, sort=False)          \n",
    "\n",
    "        return self.df_dates_total            \n",
    "            \n",
    "    def subset(self):\n",
    "        \"Determine bursts to subset by after scenes are concatenated\"\n",
    "        \n",
    "        df_subset = pd.DataFrame([]) \n",
    "\n",
    "        for x in self.dates_list:\n",
    "            date_rows = self.org_mas_bursts.loc[self.org_mas_bursts['Date'] == x]     \n",
    "\n",
    "            # check how many zip files required for date\n",
    "            zip1 = date_rows.ZipFile.unique()\n",
    "            num_zip = zip1.shape[0]            \n",
    "            \n",
    "            # check if identified zipfiles are required for all three swaths (they are required for concatenation, \n",
    "            # but may not be for subsetting to final frame)\n",
    "            IW1_rows = date_rows.loc[date_rows['Swath'] == 'IW1']\n",
    "            IW2_rows = date_rows.loc[date_rows['Swath'] == 'IW2']\n",
    "            IW3_rows = date_rows.loc[date_rows['Swath'] == 'IW3']\n",
    "            IW1_rows2 = IW1_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "            IW2_rows2 = IW2_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "            IW3_rows2 = IW3_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "    \n",
    "            # merge dataframes in two steps (can only merge two at a time)\n",
    "            merge1 = pd.merge(IW1_rows2, IW2_rows2, on=['ZipFile'], how='outer')\n",
    "            merge2 = pd.merge(merge1, IW3_rows2, on=['ZipFile'], how='outer')\n",
    "    \n",
    "            # rename columns after merging\n",
    "            merge2.columns = ['SwathIW1','ZipFile','SwathIW2','SwathIW3']\n",
    "            # reorder columns after merging\n",
    "            zip_swath_check = (merge2[['ZipFile','SwathIW1','SwathIW2','SwathIW3']]).reset_index(drop=True)\n",
    "            \n",
    "            # if single zip file for date and there are missing swaths, exclude from final list\n",
    "            if num_zip == 1 & zip_swath_check.isnull().values.any() == True:\n",
    "                pass\n",
    "            else:\n",
    "                for y in self.swath_list:\n",
    "                    swath_rows = date_rows.loc[date_rows['Swath'] == y]\n",
    "                    # check if first identified zip file is required for selected swath (if not, need to account \n",
    "                    # for this in subset)\n",
    "                    first_swath_zip = swath_rows.iloc[0]['ZipFile']\n",
    "\n",
    "                    check = zip_swath_check.loc[zip_swath_check['ZipFile'] == first_swath_zip]\n",
    "                    check_idx = check.index\n",
    "\n",
    "                    if check_idx == 1: \n",
    "                        row = self.df_archive_subset.loc[(self.df_archive_subset['Date'] == x) & \n",
    "                                                         (self.df_archive_subset['Swath'] == y) &\n",
    "                                                         (self.df_archive_subset['ZipFile'] == first_swath_zip) & \n",
    "                                                         (self.df_archive_subset['BurstNum'] == 1)]\n",
    "                        missing_zip_max_burst = row.iloc[0]['MaxBurst']\n",
    "                    else:    \n",
    "                        missing_zip_max_burst = 0\n",
    "\n",
    "                    mas_start_burst = swath_rows.iloc[0]['MasBurst']\n",
    "                    org_start_burst = swath_rows.iloc[0]['OrgBurst']\n",
    "         \n",
    "                    if mas_start_burst.find('-') == -1:\n",
    "                        # no hyphen, single burst\n",
    "                        first_mas_num = int(mas_start_burst)  \n",
    "                        first_org_num = int(org_start_burst) \n",
    "                        if first_mas_num == 1:  \n",
    "                            if num_zip == 1:\n",
    "                                subset = first_org_num \n",
    "                            else:\n",
    "                                start_subset = first_org_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                                subset = ('%s-%s') %(start_subset,end_subset)             \n",
    "                        else:\n",
    "                            if num_zip == 1:\n",
    "                                subset = first_org_num \n",
    "                            else:    \n",
    "                                start_subset = first_mas_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                                subset = ('%s-%s') %(start_subset,end_subset)                     \n",
    "                    else:\n",
    "                        # hyphen \n",
    "                        first_mas_num = int(mas_start_burst.split('-')[0])  \n",
    "                        first_org_num = int(org_start_burst.split('-')[0])  \n",
    "                        # check if org burst is 1 if not, may need to adjust subset so it starts in right place\n",
    "                        if first_mas_num == 1:  \n",
    "                            if num_zip == 1:\n",
    "                                start_subset = first_org_num \n",
    "                                end_subset = org_start_burst.split('-')[1] \n",
    "                            else:\n",
    "                                start_subset = first_org_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                            subset = ('%s-%s') %(start_subset,end_subset)\n",
    "                        else:\n",
    "                            if num_zip == 1:\n",
    "                                start_subset = first_org_num \n",
    "                                end_subset = org_start_burst.split('-')[1]\n",
    "                            else:    \n",
    "                                start_subset = first_mas_num\n",
    "                                end_subset = start_subset + 11\n",
    "                            subset = ('%s-%s') %(start_subset,end_subset)                        \n",
    "                    \n",
    "                    swath = (\"Subset%s\" % y)               \n",
    "                    df1 = pd.DataFrame([[x,subset]],columns = ['Date',swath])\n",
    "                    df_subset = df_subset.append(df1, ignore_index=True, sort=False)          \n",
    "\n",
    "            df_bursts = pd.merge(self.df_dates_total, df_subset, on=\"Date\")   \n",
    "\n",
    "        self.subset_bursts = df_bursts.groupby(['Date'])[['TotBurstsIW1','TotBurstsIW2','TotBurstsIW3',\n",
    "                                                          'SubsetIW1','SubsetIW2','SubsetIW3']].first().reset_index()\n",
    "        \n",
    "        return self.subset_bursts                \n",
    "                \n",
    "    def finalise_subset(self):\n",
    "        \"Check maximum number of bursts and determine if scene is complete for each date, finalise subset list\"\n",
    "        df_check_max = pd.DataFrame([])\n",
    "        \n",
    "        for w in self.dates_list:\n",
    "            row = self.df_dates_total.loc[self.df_dates_total['Date'] == w]\n",
    "            IW1 = int(row.iloc[0]['TotBurstsIW1'])\n",
    "            # check if complete frame (i.e. 12 bursts present)\n",
    "            if IW1 < 12:\n",
    "                complete = 'no'\n",
    "            elif IW1 == 12: \n",
    "                max_burst1 = self.subset_bursts.iloc[0]['SubsetIW1'].split('-')[0]\n",
    "                max_burst = int(max_burst1)\n",
    "                if max_burst < 12:\n",
    "                    complete = 'no'\n",
    "                else:\n",
    "                    complete = 'yes'\n",
    "            else:\n",
    "                complete = 'yes'\n",
    "                       \n",
    "            df1 = pd.DataFrame([[w, complete]],columns = ['Date','CompleteFrame'])\n",
    "            \n",
    "            df_check_max = df_check_max.append(df1, ignore_index=True, sort=False)          \n",
    "    \n",
    "        # merge final details\n",
    "        self.subset_final = pd.merge(self.subset_bursts, df_check_max, on=\"Date\")   \n",
    "        \n",
    "        return self.subset_final\n",
    "\n",
    "    \n",
    "    def centre_date(self):\n",
    "        \"Find centre date for use with resizing, DEM and slave coregistration\"\n",
    "        \n",
    "        # extract dates with complete frames only\n",
    "        complete = self.subset_final.loc[self.subset_final['CompleteFrame'] == 'yes']\n",
    "        complete_list = list(complete['Date'])\n",
    "\n",
    "        # convert to date format to find middle date\n",
    "        dates = []\n",
    "        for i in complete_list:\n",
    "            date = datetime.datetime.strptime(str(i),'%Y%m%d').strftime('%d/%m/%Y')\n",
    "            dates.append(date)\n",
    "    \n",
    "        # find middle date (copes with list with an odd numbered length)    \n",
    "        mid_line = int(len(dates))//2\n",
    "        mid_date = datetime.datetime.strptime(dates[mid_line],'%d/%m/%Y')\n",
    "        self.centre_date = mid_date.strftime('%Y%m%d')\n",
    "\n",
    "        return self.centre_date\n",
    "    \n",
    "    \n",
    "    def gamma_output(self):\n",
    "        \"Save output to text file for input into GAMMA.\"\n",
    " \n",
    "        now = datetime.datetime.now().strftime(\"%d-%b-%Y\")\n",
    "        pd.set_option('display.max_colwidth', -1) # stops concatenating dataframe contents when displayed\n",
    "        pd.options.display.float_format = '{:,.0f}'.format # changes default display of int numbers from float to int\n",
    "\n",
    "        # files to download\n",
    "        download_string = self.download_files.to_string(index=None) \n",
    "\n",
    "        # subset bursts\n",
    "        subset_string = self.subset_final.to_string(index=None) \n",
    "\n",
    "        # org versus master bursts\n",
    "        org_mas_string = self.org_mas_bursts.to_string(index=None) \n",
    "        \n",
    "        # zip file max bursts\n",
    "        max_bursts_string = self.df_max_bursts_swath.to_string(index=None)\n",
    "\n",
    "        # write results to text file\n",
    "        temp_out_file = '%s/temp_scene_stack.txt' %(self.project_dir)\n",
    "        out_file = '%s/S1_T%s%s_F%s_%s_scene_stack.txt' %(self.project_dir,self.track,self.orient2,self.frame,self.polar)\n",
    "\n",
    "        if os.path.exists(out_file):\n",
    "            os.remove(out_file)\n",
    "\n",
    "        f = open(temp_out_file, 'w+')\n",
    "        f.write(\"SENTINEL-1 SARA ARCHIVE RESULTS FOR GAMMA PROCESSING\\n\")\n",
    "        f.write(\"List created: %s\\n\" % now)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"TRACK: %s\\n\" % self.track)\n",
    "        f.write(\"FRAME: %s\\n\" % self.frame)\n",
    "        f.write(\"POLARISATION: %s\\n\" % self.polar)\n",
    "        f.write(\"ORIENTATION: %s\\n\" % self.orient)\n",
    "        f.write(\"DATE_RANGE: %s - %s\\n\" %(self.start,self.end))\n",
    "        f.write(\"\\n\")      \n",
    "        f.write(\"RESIZE_MASTER: %s\\n\" % self.first_date)\n",
    "        f.write(\"COREG_MASTER: %s\\n\" % self.centre_date)\n",
    "        f.write(\"\\n\")       \n",
    "        f.write(\"DEM_COORDINATES(minX,minY,maxX,maxY): %s\\n\" % str(self.dem_coords))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"FILES_TO_DOWNLOAD\\n\")\n",
    "        f.write(\"%s\\n\" % download_string)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"SUBSET_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % subset_string)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"ORG_BURSTS_V_MASTER_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % org_mas_string)\n",
    "        f.write(\"\\n\")      \n",
    "        f.write(\"MAX_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % max_bursts_string)\n",
    "        f.close()                            \n",
    "        \n",
    "        ## fix leading white space before dates\n",
    "        temp_out = []\n",
    "        with open(temp_out_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\" 2\"):\n",
    "                    line2 = line.lstrip()\n",
    "                    temp_out.append(line2)\n",
    "                else:\n",
    "                    temp_out.append(line)\n",
    "\n",
    "        with open(out_file, 'w') as f:\n",
    "            for line in temp_out:\n",
    "                f.write(line)\n",
    "        f.close()     \n",
    "\n",
    "        os.remove(temp_out_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = id_stack_scenes(burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period)\n",
    "obj.subset_archive()\n",
    "obj.dem_extent()\n",
    "obj.remove_duplicates() \n",
    "obj.first_full_scene()\n",
    "obj.org_mas_bursts() \n",
    "obj.max_bursts_swath() \n",
    "obj.download_files()\n",
    "obj.tot_bursts() \n",
    "obj.subset() \n",
    "obj.finalise_subset()\n",
    "obj.centre_date()\n",
    "obj.gamma_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### If errors occur, need to troubleshoot. May be due to error in input archive metadata (e.g. wrong format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below enables running functions step by step to isolate issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximise column widths when displaying dataframe results\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "obj = id_stack_scenes(burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period)\n",
    "df1 = obj.subset_archive() # self.df_archive_matching\n",
    "df_coord = obj.dem_extent() # self.dem_coords\n",
    "df2 = obj.remove_duplicates() # self.df_archive_matching_no_dups\n",
    "obj.first_full_scene()\n",
    "df3 = obj.org_mas_bursts() # self.org_mas_bursts\n",
    "df4 = obj.max_bursts_swath() # self.df_max_bursts_swath\n",
    "obj.download_files()\n",
    "df5 = obj.tot_bursts() # self.df_dates_total \n",
    "df6 = obj.subset() # self.subset_bursts\n",
    "df7 = obj.finalise_subset() #self.df_subset_final\n",
    "df8 = obj.centre_date() #self.centre_date\n",
    "#obj.gamma_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables below pre-set so if a function is extracted to be run line-by line (copied to new cell), \n",
    "# no changes to the variables are required\n",
    "\n",
    "df_archive_matching = df1\n",
    "df_archive_matching_no_dups = df2\n",
    "#df_dates_total = df5\n",
    "#org_mas_bursts = df3\n",
    "\n",
    "swaths = df_archive_matching_no_dups.MasSwath.unique()\n",
    "swath_list = swaths.tolist()\n",
    "dates = df_archive_matching_no_dups.Date.unique()\n",
    "dates_list = dates.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paste copy of function to run line by line if requried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
