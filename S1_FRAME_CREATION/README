#### Sentinel-1 archive metadata extraction, create files for use in 'frame' scene selection and create shapefiles
## developed by S Lawrie, 13 March 2019

All processing and files on NCI: /g/data1/dg9/SENTINEL-1_BURSTS (except ArcMap: DEA_collaboration/master_burst_frames/)

Scripts located in GAMMA repo 'S1_FRAME_CREATION' directory
    - the python notebooks are accessed via the VDI and therefore the repo also needs to be added to the VDI

Uses two virtual machines for python: one for NCI (miniconda) and one for VDI (virtual_envs) - can't use same VM on both systems, compatibility issues
    - refer to repo/gamma_insar/S1_FRAME_CREATION/vm_doco/ for details
    - need to update '.bash_profile' file on the VDI before using for first time



#### DIRECTORIES in /g/data1/dg9/SENTINEL-1_BURSTS ####

MASTER_BURSTS
    - master bursts are manually determined in ArcMap
    - the attribute table in ArcMap is saved as a text file and moved to this directory
    - the text files are used to create python dataframes. These dataframes are used as input for identifying scenes in the archive

ARCHIVE_TRACKS
    - contains python dataframes with the archive metadata for each track
    - these are used as input for identifying scenes in the archive for a specific track/frame
    - each time new metadata is extracted from the archive, these dataframes are automatically updated

SHAPEFILES
    - contains shapefiles showing the individual burst extents and overall scene extent for each archive scene

processing
    - contains all the intermediate files/pbs jobs used for extracting archive metadata

virtual_envs
    - contains the VM used on the VDI for the python notebooks

miniconda3
    - contains the VM used on the NCI (Raijin) for python scripts



#### CREATE MASTER BURSTS ####
  IDENTIFY MASTER BURSTS
    - use ArcMap to determine the bursts for each track/frame (DEA_collaboration/master_burst_frames/S1_master_burst_frames.mxd)
    - copy bursts from each archive track to make a new master burst frame with 12 bursts
    - if no burst exists in archive, create a blank burst and add to the blank burst shapefile
    - make sure each frame overlaps by one burst
    - need to make sure the polarisation 'Polar' attribute reflects the available data (eg. may have multiple polarisations for the same burst). Enter manually
    - create a frame extents from bursts:
        - Data Management Tools > Generalization > Dissolve.
        - select burst shapefile, save in file geodatabase, dissolve by 'RelOrbit' and 'Frame'
        - may need to manually fix some polygons as some errors during dissolve may occur. Delete vertices as required
    - export the attribute table for each shapefile (not blank burst shapefiles)
        - select all records and text file
        - move text file to NCI (MASTER_BURSTS directory)

  CREATE PYTHON DATAFRAMES
    - open VDI and run commands:
        > cd /g/data1/dg9/SENTINEL-1_BURSTS
        > module load python3/3.5.2 ipython/4.2.0-py3.5
        > source /g/data1/dg9/SENTINEL-1_BURSTS/virtual_envs/S1_Frames_VDI/py3.5.2/bin/activate
        > cd your_repo/gamma_insar/S1_FRAME_CREATION
        > jupyter notebook
    - open 'create_S1_master_burst_dataframe.ipynb' notebook
        - update the orientation details
	- run notebook line by line to create dataframe
        - repeat for each orientation



#### EXTRACT ARCHIVE METADATA ####
    - log into NCI and run the commands:
        > loadgamma
        >  cd /g/data1/dg9/SENTINEL-1_BURSTS
    - update 'S1_burst_config' file
        - today's date and processing steps
    - run each processing step separately
        > run_S1_bursts S1_burst_config
            - will create pbs jobs to extract metadata (in processing/pbs_jobs dir)

    Run in three steps. Processing steps must run each step separately, as PBS job generation for step 2 requires output list generated in step 1, and same with step 3 depending on output from step 2

    ## NOTE: run step 3 only once otherwise it will duplicate metadata in the text files. If problems running dataframe and shapefile creation, do it manually outside this workflow

    1. Create list of zip files
       - extracts a list of all the zip files in the S1 archive
       - can compare with previous list to find only new zip files added since last time this script was run

    2. Extract metadata
       - uses the zip file list to extract metadata and creates text files

    3. Create archive lists, python dataframes and shapefiles from extracted metadata
       - consolidates extracted metadata into text files
       - creates python dataframes which are used when selecting scenes for a particular 'frame'
       - creates shapefiles of burst extents for a zip file and a shapefile of the overall scene extent (merges bursts)


#### IDENTIFY SCENES FOR A TRACK/FRAME, INCLUDING ADD NEW SCENES ####
    - open VDI and run commands:
        > cd /g/data1/dg9/SENTINEL-1_BURSTS
        > module load python3/3.5.2 ipython/4.2.0-py3.5
        > source /g/data1/dg9/SENTINEL-1_BURSTS/virtual_envs/S1_Frames_VDI/py3.5.2/bin/activate
        > cd your_repo/gamma_insar/S1_FRAME_CREATION
        > jupyter notebook
    - open required notebook: 'create_S1_frame_stack_list.ipynb' or 'add_S1_frame_stack_list.ipynb'
        - update track and frame details
        - run notebook line by line to create scene text file for use by GAMMA

    ## NOTE: If any scenes have been excluded from previous processing, these need to be listed under 'remove_slcs.list'. The 'add_S1_frame_stack_list.ipynb' notebook uses this list (if present)

    ## NOTE: If any ifgss have been excluded from previous processing, these need to be listed under 'remove_ifgs.list'. The 'add_S1_frame_stack_list.ipynb' notebook uses this list (if present) 
