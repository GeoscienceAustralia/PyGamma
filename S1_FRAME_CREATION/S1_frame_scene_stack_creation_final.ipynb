{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify files in Sentinel-1 archive to create 'frame' scene stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# developed 15 Feb 2019, S Lawrie, GA\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import cv2\n",
    "import datetime\n",
    "import fiona\n",
    "\n",
    "burst_dir = \"/g/data1/dg9/SENTINEL-1_BURSTS\"\n",
    "\n",
    "%matplotlib inline \n",
    "# required for correct plotting in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Track/frame for desired scene stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = \"VICTORIA\"\n",
    "# orient: \"Ascending\" or \"Descending\"\n",
    "orient = \"Descending\"\n",
    "track = 147\n",
    "frame = 19\n",
    "polar = \"VV\"\n",
    "start_period = 20140101\n",
    "end_period = 20190213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create project processing directory\n",
    "insar_dir = \"/g/data1/dg9/INSAR_ANALYSIS\"\n",
    "project_dir = os.path.join(insar_dir,project,\"S1\",\"GAMMA\")\n",
    "kml_dir = os.path.join(project_dir,\"kmls\")\n",
    "\n",
    "# make directories\n",
    "if not os.path.exists(project_dir):\n",
    "    os.makedirs(project_dir)\n",
    "if not os.path.exists(kml_dir):\n",
    "    os.makedirs(kml_dir)\n",
    "    \n",
    "if orient == \"Ascending\":\n",
    "    orient2 = \"A\"\n",
    "elif orient == \"Descending\":\n",
    "    orient2 = \"D\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe of master burst coordinates for future use\n",
    "#### Master burst coordinates created in ArcGIS and details stored in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def master_coords():\n",
    "    \"Creates a dataframe with shapely polygons of master burst coordinates which have a buffer applied.\" \n",
    "    \"To save future processing time, the dataframe is saved for use when creating 'frame' scene stacks.\"\n",
    "    \n",
    "    # load master burst coordinates text file into a dataframe \n",
    "    df_master_input = pd.read_csv(\"%s/MASTER_BURSTS/S1_master_bursts_T89-16-118-45-147-74.txt\" %burst_dir, sep=\",\")\n",
    "\n",
    "    ## Convert coordinates into shapely polygons and put into new dataframe\n",
    "    df_master = pd.DataFrame([])  # create blank dataframe\n",
    "\n",
    "    # iterate over coordinates and append to blank dataframe\n",
    "    for i, row in df_master_input.iterrows():\n",
    "        mission = row[1]\n",
    "        type1 = row[2]\n",
    "        mode = row[3]\n",
    "        pass1 = row[4]\n",
    "        polar = row[5]\n",
    "        orbit = row[6]        \n",
    "        frame = row[7]\n",
    "        swath = row[8]\n",
    "        burst_num = row[9]\n",
    "        ul = Point(float(row[10]), float(row[11]))\n",
    "        ur = Point(float(row[12]), float(row[13]))\n",
    "        lr = Point(float(row[14]), float(row[15]))\n",
    "        ll = Point(float(row[16]), float(row[17]))\n",
    "        pointList = [ul, ur, lr, ll]\n",
    "        poly = Polygon([[p.x, p.y] for p in pointList]) # creates shapely polygon\n",
    "        df_temp1 =  gpd.GeoDataFrame([[mission,type1,mode,pass1,polar,orbit,frame,swath,burst_num,poly]],\n",
    "                          columns = ['Mission','Type','Mode','Pass','Polar','RelOrbit','Frame','Swath',\n",
    "                                     'BurstNum','Extent'],\n",
    "                          geometry='Extent')\n",
    "        df_master = df_master.append(df_temp1, ignore_index=True)\n",
    "    \n",
    "    # calculate buffer for each master burst extent and add to dataframe\n",
    "    for i in df_master.index:\n",
    "        poly = df_master.at[i,'Extent']\n",
    "        buffer = poly.buffer(+0.02, cap_style=2,join_style=2)\n",
    "        df_master.at[i,'Buffer'] = buffer\n",
    "    \n",
    "    # save master burst coordinates dataframe for future use\n",
    "    name = '%s/MASTER_BURSTS/S1_IW_SLC_%s_Master_Bursts_Dataframe' %(burst_dir,orient)\n",
    "    df = df_master.to_pickle(name)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create dataframe of archive burst coordinates for future use\n",
    "#### Archive burst coordinates created in ArcGIS and details stored in a text file, stored by relative track number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def archive_coords():\n",
    "    \"Creates a dataframe with shapely polygons of archive burst coordinates for a relative track.\" \n",
    "    \"To save future processing time, the dataframe is saved for use when creating 'frame' scene stacks.\"\n",
    "    \n",
    "    # load archive burst coordinates into a dataframe\n",
    "    input_file = '%s/ARCHIVE_TRACKS/%s_track_data/S1_IW_SLC_T%s%s_burst-metadata' %(burst_dir,orient,track,orient2)\n",
    "    df_archive_input = pd.read_csv(input_file, sep=\" \")\n",
    "    \n",
    "    # determine each swath's maximum number bursts for each zip file (used for GAMMA input)\n",
    "    df_zip = pd.DataFrame([])    \n",
    "    swaths = df_archive_input.Swath.unique()\n",
    "    swath_list = swaths.tolist()\n",
    "\n",
    "    for x in swath_list:\n",
    "        swath = df_archive_input.loc[df_archive_input['Swath'] == x]\n",
    "        zip_files = swath.ZipFile.unique()\n",
    "        zip_list = zip_files.tolist()\n",
    "    \n",
    "        for y in zip_list:\n",
    "            files = swath[swath.ZipFile == y]\n",
    "            max_burst = files['BurstNum'].max()\n",
    "            df1 = pd.DataFrame([[x, y, max_burst]],columns = ['Swath','ZipFile','MaxBurst'])\n",
    "            df_zip = df_zip.append(df1, ignore_index=True)      \n",
    "        \n",
    "    ## Convert archive burst coordinates into shapely polygons and put into new dataframe\n",
    "    df_archive = pd.DataFrame([]) # create blank dataframe\n",
    "   \n",
    "    # iterate over coordinates and append to blank dataframe    \n",
    "    for i, row in df_archive_input.iterrows():\n",
    "        mission = row[0]\n",
    "        type1 = row[2]\n",
    "        date = row[3]\n",
    "        pass1 = row[4]\n",
    "        polar = row[5]\n",
    "        rel_orbit = row[7]\n",
    "        swath = row[8]\n",
    "        burst_num = row[9]             \n",
    "        ipf = row[17]\n",
    "        raw_date = row[19]    \n",
    "        ul = Point(row[26], row[27])\n",
    "        ur = Point(row[28], row[29])\n",
    "        lr = Point(row[30], row[31]) \n",
    "        ll = Point(row[32], row[33])\n",
    "        grid = row[35]\n",
    "        zip_file = row[36]\n",
    "        pointList = [ul, ur, lr, ll]\n",
    "        poly = Polygon([[p.x, p.y] for p in pointList]) # creates shapely polygon\n",
    "        df_temp2 =  gpd.GeoDataFrame([[mission,type1,date,pass1,polar,rel_orbit,ipf,raw_date,burst_num,swath,\n",
    "                                       poly,grid,zip_file]],\n",
    "                                     columns = ['Mission','Type','Date','Pass','Polar','RelOrbit','IPFVer','RawDate',\n",
    "                                                'BurstNum','Swath','Extent','GridDir','ZipFile'],\n",
    "                                     geometry='Extent')\n",
    "        df_archive = df_archive.append(df_temp2, ignore_index=True)\n",
    "    \n",
    "    # add max burst value to dataframe\n",
    "    df_archive2 = pd.merge(df_archive, df_zip, on=['Swath','ZipFile'])\n",
    "    \n",
    "    # save updated archive burst coordinates dataframe for future use\n",
    "    out_file = '%s/ARCHIVE_TRACKS/%s_track_data/S1_IW_SLC_T%s%s_Dataframe' %(burst_dir,orient,track,orient2)\n",
    "    df = df_archive2.to_pickle(out_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset saved dataframes to desired track/frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class id_stack_scenes:\n",
    "    def __init__(self,burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period):\n",
    "        self.burst_dir = burst_dir\n",
    "        self.project_dir = project_dir\n",
    "        self.kml_dir = kml_dir\n",
    "        self.orient = orient\n",
    "        self.orient2 = orient2\n",
    "        self.track = track\n",
    "        self.frame = frame\n",
    "        self.polar = polar\n",
    "        self.start = start_period\n",
    "        self.end = end_period\n",
    "        \n",
    "    def subset_archive(self):\n",
    "        \"Load saved dataframes and subset them by orientation, polarisation, track and frame.\" \n",
    "        \"Find which archive bursts fit within the master burst buffer coordinates.\"\n",
    "        \n",
    "        # load saved master burst coordinates dataframe\n",
    "        input_master = '%s/MASTER_BURSTS/S1_IW_SLC_%s_Master_Bursts_Dataframe' %(self.burst_dir,self.orient)\n",
    "        self.df_master = pd.read_pickle(input_master)\n",
    "        \n",
    "        # load saved archive burst coordinates dataframe\n",
    "        input_archive = '%s/ARCHIVE_TRACKS/%s_track_data/S1_IW_SLC_T%s%s_Dataframe' %(self.burst_dir,self.orient,self.track,self.orient2)\n",
    "        self.df_archive = pd.read_pickle(input_archive)\n",
    "        \n",
    "        # subset master burst coordinates to desired scene stack\n",
    "        self.df_master_subset = self.df_master.loc[(self.df_master['Pass'] == self.orient) & \n",
    "                                                   (self.df_master['RelOrbit'] == self.track) & \n",
    "                                                   (self.df_master['Frame'] == self.frame) & \n",
    "                                                   (self.df_master['Polar'].str.contains(self.polar))]\n",
    "\n",
    "        # create KML file of master frame\n",
    "        master_merged = self.df_master_subset.dissolve(by='Mode') # merge all polygons into one\n",
    "        master_merged.reset_index(level=0, inplace=True) \n",
    "        master_merged2 = master_merged.drop(columns=['Buffer']) # remove buffer column to enable correct coordinates to be used\n",
    "        fiona.supported_drivers['KML'] = 'rw'\n",
    "        out_kml = '%s/S1_T%s%s_F%s_%s.kml' %(self.kml_dir,self.track,self.orient2,self.frame,self.polar)\n",
    "        master_merged2.to_file(out_kml, driver='KML')\n",
    "        \n",
    "        # subset archive burst coordinates to desired track by polarisation and date range\n",
    "        df_subset = self.df_archive.loc[(self.df_archive['Polar'] == self.polar)]\n",
    "        self.df_archive_subset = df_subset[df_subset['Date'].between(start_period, end_period, inclusive=True)]\n",
    "\n",
    "        # create blank dataframe to save matching bursts in archive\n",
    "        self.df_archive_matching = pd.DataFrame([]) \n",
    "        \n",
    "        # select archive bursts that fall within master bursts\n",
    "        swaths = self.df_master_subset.Swath.unique()\n",
    "        self.swath_list = swaths.tolist()\n",
    "    \n",
    "        for x in self.swath_list:\n",
    "            swath_bursts = self.df_master_subset.loc[self.df_master_subset['Swath'] == x]\n",
    "            bursts = swath_bursts['BurstNum']\n",
    "            burst_list = bursts.tolist()  \n",
    "    \n",
    "            for y in burst_list:\n",
    "                burst_row = swath_bursts.loc[swath_bursts['BurstNum'] == y].reset_index(drop=True)\n",
    "                buffer = burst_row['Buffer'][0]\n",
    "                frame = burst_row['Frame'][0]\n",
    "                mas_burst_num = burst_row['BurstNum'][0]\n",
    "                mas_swath = burst_row['Swath'][0]\n",
    "\n",
    "                # iterate over archive and append matching bursts to blank dataframe\n",
    "                for i, row in self.df_archive_subset.iterrows():\n",
    "                    poly = row['Extent']\n",
    "                    check = buffer.contains(poly)\n",
    "                    if check == True:\n",
    "                        date = row[2]\n",
    "                        pass1 = row[3]\n",
    "                        polar = row[4]\n",
    "                        rel_orbit = row[5]\n",
    "                        ipf = row[6]\n",
    "                        raw_date = row[7]\n",
    "                        org_burst_num = row[8]\n",
    "                        org_swath = row[9]\n",
    "                        grid = row[11]\n",
    "                        zip_file = row[12]\n",
    "                        org_max_burst = row[13]  \n",
    "                        df_temp = gpd.GeoDataFrame([[date,pass1,polar,rel_orbit,frame,ipf,raw_date,org_swath,\n",
    "                                                     org_burst_num,org_max_burst,mas_swath,mas_burst_num,poly,\n",
    "                                                     grid,zip_file]],\n",
    "                                                   columns = ['Date','Pass','Polar','RelOrbit','Frame','IPFVer',\n",
    "                                                              'RawDate','OrgSwath','OrgBurstNum','OrgMaxBurst',\n",
    "                                                              'MasSwath','MasBurstNum','Extent','GridDir','ZipFile'],\n",
    "                                                   geometry='Extent')                    \n",
    "                        self.df_archive_matching = self.df_archive_matching.append(df_temp, ignore_index=True)    \n",
    "\n",
    "        return self.df_archive_matching\n",
    "        \n",
    "    def dem_extent(self):        \n",
    "        \"Use master burst coordinates to determine DEM extent for auto DEM generation in GAMMA.\"       \n",
    "        \n",
    "        # merge burst polygons to single polygon (using burst coords)\n",
    "        buffer_coords = self.df_master_subset[['Mission','Buffer']]\n",
    "        polys1 = gpd.GeoDataFrame(buffer_coords,geometry='Buffer') \n",
    "        polys2 = polys1.dissolve(by='Mission')\n",
    "        minx = polys2.bounds.at['S1','minx']\n",
    "        miny = polys2.bounds.at['S1','miny']\n",
    "        maxx = polys2.bounds.at['S1','maxx']\n",
    "        maxy = polys2.bounds.at['S1','maxy']\n",
    "\n",
    "        # create new dataframe with overall frame coords\n",
    "        ul = Point(minx,maxy)\n",
    "        ur = Point(maxx,maxy)\n",
    "        lr = Point(maxx,miny)\n",
    "        ll = Point(minx,miny)\n",
    "        pointList = [ul, ur, lr, ll]\n",
    "        pol2 = Polygon([[p.x, p.y] for p in pointList]) # creates shapely polygon\n",
    "        frame_extent = gpd.GeoDataFrame([[pol2]],columns = ['Extent'],geometry='Extent')\n",
    "\n",
    "        # add additional buffer to frame extent\n",
    "        poly = frame_extent.at[0,'Extent']\n",
    "        frame_buffer = poly.buffer(+0.3, cap_style=2,join_style=2)\n",
    "        frame_extent.at[0,'Buffer'] = frame_buffer\n",
    "        frame2 =  gpd.GeoDataFrame(frame_extent,geometry='Buffer')\n",
    "\n",
    "        # extract coords for text file\n",
    "        fr_minx = frame_extent.bounds.at[0,'minx']\n",
    "        fr_miny = frame_extent.bounds.at[0,'miny']\n",
    "        fr_maxx = frame_extent.bounds.at[0,'maxx']\n",
    "        fr_maxy = frame_extent.bounds.at[0,'maxy']\n",
    "        self.dem_coords = fr_minx,fr_miny,fr_maxx,fr_maxy\n",
    "        \n",
    "        return self.dem_coords    \n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"Identifies and removes duplicates.\"  \n",
    "\n",
    "        # add temporary index column for this step\n",
    "        self.df_archive_matching['idx'] = self.df_archive_matching.index\n",
    "\n",
    "        scene_date_dups = self.df_archive_matching[self.df_archive_matching.duplicated(subset=['Date','MasSwath',\n",
    "                                                                                               'MasBurstNum'],\n",
    "                                                                                       keep=False)]\n",
    "        # 1: identify all unique dates (i.e. no duplicates)\n",
    "        dup_scene_date_index = scene_date_dups.index.tolist() \n",
    "        df_uniq = self.df_archive_matching\n",
    "        df_uniq.drop(df_uniq.index[dup_scene_date_index], inplace = True)\n",
    "\n",
    "        # 2: identify remaining duplicates that have same date but diff IPF version\n",
    "        ipf_max = scene_date_dups.groupby('Date').IPFVer.transform(max)\n",
    "        ipf_dups = scene_date_dups.loc[scene_date_dups.IPFVer == ipf_max]\n",
    "        ipf_uniq = ipf_dups.drop_duplicates(subset=['Date','IPFVer','MasSwath','MasBurstNum'], keep=False)\n",
    "\n",
    "        # 3: identify remaining duplicates that have same date and IPF version but diff raw date\n",
    "        ipf_uniq_index = ipf_uniq.index.tolist()\n",
    "        raw_date_dups1 = ipf_dups[~ipf_dups['idx'].isin(ipf_uniq_index)]\n",
    "        raw_date_max = raw_date_dups1.groupby('Date').RawDate.transform(max)\n",
    "        raw_date_dups = raw_date_dups1.loc[raw_date_dups1.RawDate == raw_date_max]\n",
    "        raw_date_uniq = raw_date_dups.drop_duplicates(subset=['Date','RawDate','MasSwath','MasBurstNum'],\n",
    "                                                      keep=False)\n",
    "\n",
    "        # 4: remove remaining duplcates (i.e. all details the same, keep one row)\n",
    "        raw_date_uniq_index = raw_date_uniq.index.tolist()\n",
    "        remain_date_dups1 = raw_date_dups[~raw_date_dups['idx'].isin(raw_date_uniq_index)]\n",
    "        remain_date_uniq = remain_date_dups1.drop_duplicates(subset = ['Date','IPFVer','RawDate','MasSwath',\n",
    "                                                                       'MasBurstNum'],keep='first')\n",
    "\n",
    "        # 5: join all unique dataframes together to get final matching archive dataframe\n",
    "        df_no_dups1 = pd.concat([df_uniq,ipf_uniq,raw_date_uniq,remain_date_uniq],axis=0,sort=False).reset_index(drop=True)\n",
    "        df_no_dups = df_no_dups1.drop(columns='idx',axis=1)\n",
    "       \n",
    "        # 6: sort final dataframe\n",
    "        self.df_archive_matching_no_dups = df_no_dups.sort_values(['Date','MasSwath','MasBurstNum'],\n",
    "                                                                  ascending=[True,True,True])  \n",
    "     \n",
    "        return self.df_archive_matching_no_dups\n",
    "        \n",
    "    def first_full_scene(self):      ##### WILL NEED MODIFYING TO DEAL WITH ALL SCENES WHICH DON'T HAVE 12 BURSTS\n",
    "        \"Identify first scene which has 12 master bursts (use for resizing any scenes which don't have 12 bursts)\"\n",
    "    \n",
    "        dates = self.df_archive_matching_no_dups.Date.unique()\n",
    "        self.dates_list = dates.tolist()\n",
    "        df_dates = pd.DataFrame([]) \n",
    "    \n",
    "        for x in self.dates_list:\n",
    "            iw1_b1 = self.df_archive_matching_no_dups.loc[(self.df_archive_matching_no_dups['Date'] == x) &\n",
    "                                                          (self.df_archive_matching_no_dups['MasSwath'] == 'IW1') & \n",
    "                                                          (self.df_archive_matching_no_dups['MasBurstNum'] == 1)]\n",
    "            iw1_b12 = self.df_archive_matching_no_dups.loc[(self.df_archive_matching_no_dups['Date'] == x) & \n",
    "                                                           (self.df_archive_matching_no_dups['MasSwath'] == 'IW1') & \n",
    "                                                           (self.df_archive_matching_no_dups['MasBurstNum'] == 12)]  \n",
    "\n",
    "            # check if results are returned for both bursts 1 and 12, if append dates to new dataframe\n",
    "            if iw1_b1.empty or iw1_b12.empty:\n",
    "                pass\n",
    "            else:\n",
    "                df_dates = df_dates.append(iw1_b1,ignore_index=True)\n",
    "                df_dates = df_dates.append(iw1_b12,ignore_index=True)\n",
    "        df1 = df_dates.sort_values(by=['Date'])\n",
    "        self.first_date = df1['Date'].iloc[0]\n",
    "\n",
    "    def org_mas_bursts(self):\n",
    "        \"Summarise the original bursts and the equivalant master burst numbers\"\n",
    "\n",
    "        self.org_mas_bursts = pd.DataFrame([]) \n",
    "\n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_archive_matching_no_dups.loc[self.df_archive_matching_no_dups['Date'] == w]  \n",
    "            for x in self.swath_list:\n",
    "                swath_rows = date_rows.loc[date_rows['MasSwath'] == x]\n",
    "                zip_files = swath_rows.ZipFile.unique()\n",
    "                zip_list = zip_files.tolist() \n",
    "                for z in zip_list:\n",
    "                    rows = swath_rows.loc[swath_rows['ZipFile'] == z]     \n",
    "                    grid = rows.iloc[0,13]      \n",
    "                    min_org_burst = rows.OrgBurstNum.min()\n",
    "                    max_org_burst = rows.OrgBurstNum.max()\n",
    "                    if min_org_burst == max_org_burst:\n",
    "                        org_burst = ('%s' % min_org_burst)\n",
    "                    else:\n",
    "                        org_burst = ('%s-%s') %(min_org_burst,max_org_burst)\n",
    "                    min_mas_burst = rows.MasBurstNum.min()\n",
    "                    max_mas_burst = rows.MasBurstNum.max()\n",
    "                    if min_mas_burst == max_mas_burst:\n",
    "                        mas_burst = ('%s' % min_mas_burst)\n",
    "                    else:\n",
    "                        mas_burst = ('%s-%s') %(min_mas_burst,max_mas_burst)\n",
    "    \n",
    "                    df1 = pd.DataFrame([[w,x,org_burst,mas_burst,grid,z]],columns = ['Date','Swath','OrgBurst',\n",
    "                                                                                     'MasBurst','GridDir','ZipFile'])\n",
    "                    self.org_mas_bursts = self.org_mas_bursts.append(df1, ignore_index=True, sort=False)              \n",
    "\n",
    "        return self.org_mas_bursts\n",
    "            \n",
    "    def max_bursts_swath(self):  \n",
    "        \"Determine the maximum bursts per swath for each zip file\"\n",
    "        \n",
    "        df_max_bursts_swath1 = pd.DataFrame([]) \n",
    "        \n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_archive_matching_no_dups.loc[self.df_archive_matching_no_dups['Date'] == w]     \n",
    "            date_zip_files = date_rows.ZipFile.unique()\n",
    "            date_zip_list = date_zip_files.tolist()\n",
    "\n",
    "            for x in self.swath_list:\n",
    "                swath_zip = self.df_archive.loc[self.df_archive['Swath'] == x]\n",
    "                zip_files = swath_zip.ZipFile.unique()\n",
    "                zip_list = zip_files.tolist() \n",
    "\n",
    "                for y in date_zip_list:\n",
    "                    for z in zip_list:\n",
    "                        if y == z:\n",
    "                            rows = swath_zip.loc[swath_zip['ZipFile'] == y]\n",
    "                            grid = rows.iloc[0,11]\n",
    "                            max_burst = rows['MaxBurst'].max()\n",
    "                            swath = (\"MaxBurst%s\" % x)\n",
    "                            df1 = pd.DataFrame([[w, grid, z, max_burst]],columns = ['Date','GridDir','ZipFile', swath])\n",
    "                            df_max_bursts_swath1 = df_max_bursts_swath1.append(df1,ignore_index=True,sort=False)     \n",
    "\n",
    "            self.df_max_bursts_swath = df_max_bursts_swath1.groupby(['Date','GridDir',\n",
    "                                                                         'ZipFile'])[['MaxBurstIW1',\n",
    "                                                                                      'MaxBurstIW2',\n",
    "                                                                                      'MaxBurstIW3'\n",
    "                                                                                     ]].first().reset_index()   \n",
    "        return self.df_max_bursts_swath\n",
    "            \n",
    "    def download_files(self):\n",
    "        \"Create list of files to download\"\n",
    "        \n",
    "        self.download_files = self.df_max_bursts_swath[['Date','GridDir','ZipFile']]\n",
    "           \n",
    "    def tot_bursts(self):\n",
    "        \"Determine total number of bursts by swath for each date\"\n",
    "\n",
    "        self.df_dates_total = pd.DataFrame([]) \n",
    "    \n",
    "        for w in self.dates_list:\n",
    "            date_rows = self.df_max_bursts_swath.loc[self.df_max_bursts_swath['Date'] == w]\n",
    "            IW1_sum = date_rows['MaxBurstIW1'].sum()\n",
    "            IW2_sum = date_rows['MaxBurstIW2'].sum()\n",
    "            IW3_sum = date_rows['MaxBurstIW3'].sum()\n",
    "                                   \n",
    "            df1 = pd.DataFrame([[w, IW1_sum, IW2_sum, IW3_sum]],columns = ['Date','TotBurstsIW1','TotBurstsIW2',\n",
    "                                                                           'TotBurstsIW3'])\n",
    "            self.df_dates_total = self.df_dates_total.append(df1, ignore_index=True, sort=False)          \n",
    "\n",
    "        return self.df_dates_total            \n",
    "            \n",
    "    def subset(self):\n",
    "        \"Determine bursts to subset by after scenes are concatenated\"\n",
    "        \n",
    "        df_subset = pd.DataFrame([]) \n",
    "\n",
    "        for x in self.dates_list:\n",
    "            date_rows = self.org_mas_bursts.loc[self.org_mas_bursts['Date'] == x]     \n",
    "\n",
    "            # check how many zip files required for date\n",
    "            zip1 = date_rows.ZipFile.unique()\n",
    "            num_zip = zip1.shape[0]            \n",
    "            \n",
    "            # check if identified zipfiles are required for all three swaths (they are required for concatenation, \n",
    "            # but may not be for subsetting to final frame)\n",
    "            IW1_rows = date_rows.loc[date_rows['Swath'] == 'IW1']\n",
    "            IW2_rows = date_rows.loc[date_rows['Swath'] == 'IW2']\n",
    "            IW3_rows = date_rows.loc[date_rows['Swath'] == 'IW3']\n",
    "            IW1_rows2 = IW1_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "            IW2_rows2 = IW2_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "            IW3_rows2 = IW3_rows.drop(['Date','OrgBurst','MasBurst','GridDir'], axis=1)\n",
    "    \n",
    "            # merge dataframes in two steps (can only merge two at a time)\n",
    "            merge1 = pd.merge(IW1_rows2, IW2_rows2, on=['ZipFile'], how='outer')\n",
    "            merge2 = pd.merge(merge1, IW3_rows2, on=['ZipFile'], how='outer')\n",
    "    \n",
    "            # rename columns after merging\n",
    "            merge2.columns = ['SwathIW1','ZipFile','SwathIW2','SwathIW3']\n",
    "            # reorder columns after merging\n",
    "            zip_swath_check = (merge2[['ZipFile','SwathIW1','SwathIW2','SwathIW3']]).reset_index(drop=True)\n",
    "            \n",
    "            # if single zip file for date and there are missing swaths, exclude from final list\n",
    "            if num_zip == 1 & zip_swath_check.isnull().values.any() == True:\n",
    "                pass\n",
    "            else:\n",
    "                for y in self.swath_list:\n",
    "                    swath_rows = date_rows.loc[date_rows['Swath'] == y]\n",
    "                    # check if first identified zip file is required for selected swath (if not, need to account \n",
    "                    # for this in subset)\n",
    "                    first_swath_zip = swath_rows.iloc[0]['ZipFile']\n",
    "\n",
    "                    check = zip_swath_check.loc[zip_swath_check['ZipFile'] == first_swath_zip]\n",
    "                    check_idx = check.index\n",
    "\n",
    "                    if check_idx == 1: \n",
    "                        row = self.df_archive_subset.loc[(self.df_archive_subset['Date'] == x) & \n",
    "                                                         (self.df_archive_subset['Swath'] == y) &\n",
    "                                                         (self.df_archive_subset['ZipFile'] == first_swath_zip) & \n",
    "                                                         (self.df_archive_subset['BurstNum'] == 1)]\n",
    "                        missing_zip_max_burst = row.iloc[0]['MaxBurst']\n",
    "                    else:    \n",
    "                        missing_zip_max_burst = 0\n",
    "\n",
    "                    mas_start_burst = swath_rows.iloc[0]['MasBurst']\n",
    "                    org_start_burst = swath_rows.iloc[0]['OrgBurst']\n",
    "         \n",
    "                    if mas_start_burst.find('-') == -1:\n",
    "                        # no hyphen, single burst\n",
    "                        first_mas_num = int(mas_start_burst)  \n",
    "                        first_org_num = int(org_start_burst) \n",
    "                        if first_mas_num == 1:  \n",
    "                            if num_zip == 1:\n",
    "                                subset = first_org_num \n",
    "                            else:\n",
    "                                start_subset = first_org_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                                subset = ('%s-%s') %(start_subset,end_subset)             \n",
    "                        else:\n",
    "                            if num_zip == 1:\n",
    "                                subset = first_org_num \n",
    "                            else:    \n",
    "                                start_subset = first_mas_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                                subset = ('%s-%s') %(start_subset,end_subset)                     \n",
    "                    else:\n",
    "                        # hyphen \n",
    "                        first_mas_num = int(mas_start_burst.split('-')[0])  \n",
    "                        first_org_num = int(org_start_burst.split('-')[0])  \n",
    "                        # check if org burst is 1 if not, may need to adjust subset so it starts in right place\n",
    "                        if first_mas_num == 1:  \n",
    "                            if num_zip == 1:\n",
    "                                start_subset = first_org_num \n",
    "                                end_subset = org_start_burst.split('-')[1] \n",
    "                            else:\n",
    "                                start_subset = first_org_num + missing_zip_max_burst\n",
    "                                end_subset = start_subset + 11\n",
    "                            subset = ('%s-%s') %(start_subset,end_subset)\n",
    "                        else:\n",
    "                            if num_zip == 1:\n",
    "                                start_subset = first_org_num \n",
    "                                end_subset = org_start_burst.split('-')[1]\n",
    "                            else:    \n",
    "                                start_subset = first_mas_num\n",
    "                                end_subset = start_subset + 11\n",
    "                            subset = ('%s-%s') %(start_subset,end_subset)                        \n",
    "                    \n",
    "                    swath = (\"Subset%s\" % y)               \n",
    "                    df1 = pd.DataFrame([[x,subset]],columns = ['Date',swath])\n",
    "                    df_subset = df_subset.append(df1, ignore_index=True, sort=False)          \n",
    "\n",
    "            df_bursts = pd.merge(self.df_dates_total, df_subset, on=\"Date\")   \n",
    "\n",
    "        self.subset_bursts = df_bursts.groupby(['Date'])[['TotBurstsIW1','TotBurstsIW2','TotBurstsIW3',\n",
    "                                                          'SubsetIW1','SubsetIW2','SubsetIW3']].first().reset_index()\n",
    "        \n",
    "        return self.subset_bursts                \n",
    "                \n",
    "    def finalise_subset(self):\n",
    "        \"Check maximum number of bursts and determine if scene is complete for each date, finalise subset list\"\n",
    "        df_check_max = pd.DataFrame([])\n",
    "        \n",
    "        for w in self.dates_list:\n",
    "            row = self.df_dates_total.loc[self.df_dates_total['Date'] == w]\n",
    "            IW1 = int(row.iloc[0]['TotBurstsIW1'])\n",
    "            # check if complete frame (i.e. 12 bursts present)\n",
    "            if IW1 < 12:\n",
    "                complete = 'no'\n",
    "            elif IW1 == 12: \n",
    "                max_burst1 = self.subset_bursts.iloc[0]['SubsetIW1'].split('-')[0]\n",
    "                max_burst = int(max_burst1)\n",
    "                if max_burst < 12:\n",
    "                    complete = 'no'\n",
    "                else:\n",
    "                    complete = 'yes'\n",
    "            else:\n",
    "                complete = 'yes'\n",
    "                       \n",
    "            df1 = pd.DataFrame([[w, complete]],columns = ['Date','CompleteFrame'])\n",
    "            \n",
    "            df_check_max = df_check_max.append(df1, ignore_index=True, sort=False)          \n",
    "    \n",
    "        # merge final details\n",
    "        self.subset_final = pd.merge(self.subset_bursts, df_check_max, on=\"Date\")   \n",
    "        \n",
    "        return self.subset_final\n",
    "        \n",
    "    def gamma_output(self):\n",
    "        \"Save output to text file for input into GAMMA.\"\n",
    " \n",
    "        now = datetime.datetime.now().strftime(\"%d-%b-%Y\")\n",
    "        pd.set_option('display.max_colwidth', -1) # stops concatenating dataframe contents when displayed\n",
    "        pd.options.display.float_format = '{:,.0f}'.format # changes default display of int numbers from float to int\n",
    "\n",
    "        # files to download\n",
    "        download_string = self.download_files.to_string(index=None) \n",
    "\n",
    "        # subset bursts\n",
    "        subset_string = self.subset_final.to_string(index=None) \n",
    "\n",
    "        # org versus master bursts\n",
    "        org_mas_string = self.org_mas_bursts.to_string(index=None) \n",
    "        \n",
    "        # zip file max bursts\n",
    "        max_bursts_string = self.df_max_bursts_swath.to_string(index=None)\n",
    "\n",
    "        # write results to text file\n",
    "        out_file = '%s/S1_T%s%s_F%s_%s_scene_stack.txt' %(self.project_dir,self.track,self.orient2,self.frame,self.polar)\n",
    "\n",
    "        if os.path.exists(out_file):\n",
    "            os.remove(out_file)\n",
    "\n",
    "        f = open(out_file, 'w+')\n",
    "        f.write(\"SENTINEL-1 SARA ARCHIVE RESULTS FOR GAMMA PROCESSING\\n\")\n",
    "        f.write(\"List created: %s\\n\" % now)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"TRACK: %s\\n\" % self.track)\n",
    "        f.write(\"FRAME: %s\\n\" % self.frame)\n",
    "        f.write(\"POLARISATION: %s\\n\" % self.polar)\n",
    "        f.write(\"ORIENTATION: %s\\n\" % self.orient)\n",
    "        f.write(\"DATE_RANGE: %s - %s\\n\" %(self.start,self.end))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"RESIZE_MASTER: %s\\n\" % self.first_date)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"DEM_COORDINATES(minX,minY,maxX,maxY): %s\\n\" % str(self.dem_coords))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"FILES_TO_DOWNLOAD\\n\")\n",
    "        f.write(\"%s\\n\" % download_string)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"SUBSET_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % subset_string)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"ORG_BURSTS_V_MASTER_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % org_mas_string)\n",
    "        f.write(\"\\n\")      \n",
    "        f.write(\"MAX_BURSTS\\n\")\n",
    "        f.write(\"%s\\n\" % max_bursts_string)\n",
    "        f.close()                            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only need to run once as output is saved\n",
    "master_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only need to run if new zip files have been added to existing track metadata list\n",
    "archive_coords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset dataframes to desired track/frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = id_stack_scenes(burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period)\n",
    "obj.subset_archive()\n",
    "obj.dem_extent()\n",
    "obj.remove_duplicates() \n",
    "obj.first_full_scene()\n",
    "obj.org_mas_bursts() \n",
    "obj.max_bursts_swath() \n",
    "obj.download_files()\n",
    "obj.tot_bursts() \n",
    "obj.subset() \n",
    "obj.finalise_subset()\n",
    "obj.gamma_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### for troubleshooting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj = id_stack_scenes(burst_dir,project_dir,kml_dir,orient,orient2,track,frame,polar,start_period,end_period)\n",
    "df1 = obj.subset_archive() # self.df_archive_matching\n",
    "df_coord = obj.dem_extent() # self.dem_coords\n",
    "df2 = obj.remove_duplicates() # self.df_archive_matching_no_dups\n",
    "#obj.first_full_scene()\n",
    "#df3 = obj.org_mas_bursts() # self.org_mas_bursts\n",
    "#df4 = obj.max_bursts_swath() # self.df_max_bursts_swath\n",
    "#obj.download_files()\n",
    "#df5 = obj.tot_bursts() # self.df_dates_total \n",
    "#df6 = obj.subset() # self.subset_bursts\n",
    "#df7 = obj.finalise_subset() #self.df_subset_final\n",
    "#obj.gamma_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_archive_matching = df1\n",
    "df_archive_matching_no_dups = df2\n",
    "#df_dates_total = df5\n",
    "#org_mas_bursts = df3\n",
    "\n",
    "swaths = df_archive_matching_no_dups.MasSwath.unique()\n",
    "swath_list = swaths.tolist()\n",
    "dates = df_archive_matching_no_dups.Date.unique()\n",
    "dates_list = dates.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
