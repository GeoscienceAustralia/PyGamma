#!/bin/bash

display_usage() {
    echo ""
    echo "*******************************************************************************"
    echo "* run_S1_bursts: Scrapes Sentinel-1 metadata from the Regional Copernicus     *"
    echo "*                archive. Calculates burst corner coordinates for each file   *"
    echo "*                so scenes can be automatically selected to create new scene  *"
    echo "*                stacks using GA's Sentinel-1 framing system.                 *"
    echo "*                                                                             *"
    echo "*                Also Creates dataframes of burst metadata for each track     *"
    echo "*                and shapefiles of burst extents and scene extents for each   *"
    echo "*                zip file in the archive.                                     *"
    echo "*                                                                             *"
    echo "*        Run script from: /g/data1/dg9/SENTINEL-1_BURSTS                      *"
    echo "*                                                                             *"
    echo "*   Automatically calls a miniconda virtual environment to run python         *"
    echo "*   scripts. To access VM manually:                                           *"
    echo "*      > source /g/data1/dg9/sll547/miniconda3/etc/profile.d/conda.sh         *"
    echo "*      > conda activate S1_Frames                                             *"
    echo "*                                                                             *"
    echo "* input:  [config_file]   name of config file (e.g. S1_burst_config)          *"
    echo "*                                                                             *"
    echo "* author: Sarah Lawrie @ GA       13/03/2019, v1.0                            *"
    echo "*******************************************************************************"
    echo -e "Usage: run_S1_bursts [config_file]"
    }

if [ $# -lt 1 ]
then
    display_usage
    exit 1
fi

config_file=$1

# Load generic variables
source ~/repo/gamma_insar/S1_FRAME_CREATION/S1_burst_functions $config_file

cd $proj_dir

mkdir -p $processing_dir $zip_dir $burst_dir $ind_burst_dir $pbs_dir $shp_dir


## Get list of zip files currently in the Sentinel-1 archive
if [ $create_zip_list == 'yes' ]; then
    if [ $compare_zip == 'yes' ]; then
	echo "Creating list of zip files added to Sentinel-1 archive since last time this script was run ..."
    else
	echo "Creating list of all zip files in Sentinel-1 archive ..."
    fi
    cd $pbs_dir
    job=s1_zip_list
    echo \#\!/bin/bash > $job
    echo \#\PBS -l other=gdata1 >> $job
    echo \#\PBS -l walltime=08:00:00 >> $job
    echo \#\PBS -l mem=10GB >> $job
    echo \#\PBS -l ncpus=1 >> $job
    echo \#\PBS -l wd >> $job
    echo \#\PBS -q express >> $job
    echo ~/repo/gamma_insar/S1_FRAME_CREATION/create_S1_zipfile_list.bash $proj_dir/$config_file >> $job
    chmod +x $job
    qsub $job 
else
    echo "Option to create zip lists not selected."
fi


## Extract metadata for each zip file and calculate burst corner coordinates
if [ $extract_metadata == 'yes' ]; then
    echo "Extracting metadata from zip files ..."

    mkdir -p $split_dir
    cd $list_dir

    # split zip file list into 5,000 blocks to speed up processing
    split -l 5000 $zip_list $split_name
    mv $split_name* $split_dir
    cd $split_dir
    ls $split_name* > $split_list

    # create PBS jobs for extracting burst metadata
    cd $pbs_dir

    while read list; do
	part=`echo $list | cut -d '_' -f 5`
	job="bursts_"$part
	echo \#\!/bin/bash > $job
	echo \#\PBS -lother=gdata1 >> $job
	echo \#\PBS -l walltime=12:00:00 >> $job
	echo \#\PBS -l mem=5GB >> $job
	echo \#\PBS -l ncpus=1 >> $job
	echo \#\PBS -l wd >> $job
	echo \#\PBS -q normal >> $job
	echo ~/repo/gamma_insar/S1_FRAME_CREATION/extract_S1_bursts_metadata.bash $proj_dir/$config_file $split_dir/$list >> $job
	chmod +x $job
	qsub $job | tee $job"_job_id"
        ls $job"_job_id" >> metadata_list
    done < $split_list

    # create dependency list
    while read id; do
        less $id >> list2
        rm -rf $id
    done < metadata_list
    sed s/.r-man2// list2 > list3 # leave just job numbers
    sort -n list3 > list4 # sort numbers
    tr '\n' ':' < list4 > list5 # move column to single row with numbers separated by :
    sed s'/.$//' list5 > all_metadata_job_ids # remove last :
    rm -rf metadata_list list2 list3 list4 list5

    # create PBS job for consolidating metadata
    job=consol_meta
    echo \#\!/bin/bash > $job
    echo \#\PBS -l other=gdata1 >> $job
    echo \#\PBS -l walltime=00:15:00 >> $job
    echo \#\PBS -l mem=500MB >> $job
    echo \#\PBS -l ncpus=1 >> $job
    echo \#\PBS -l wd >> $job
    echo \#\PBS -q express >> $job
    depend_job=`awk '{print $1}' all_metadata_job_ids`
    echo \#\PBS -W depend=afterok:$depend_job >> $job
    echo ~/repo/gamma_insar/S1_FRAME_CREATION/consolidate_metadata.bash $proj_dir/$config_file >> $job
    chmod +x $job
    qsub $job 

    echo "Archive tracks last updated on "$date > $proj_dir/LAST_UPDATE
else
    echo "Option to extract metadata from zip files not selected."
fi


## Create archive lists, dataframes and shapefiles  
if [ $create_archive == 'yes' ]; then
    echo "Creating archive lists, dataframes and shapefiles from extracted metadata ..."

    cd $pbs_dir

    # create PBS jobs for creating archive dataframes and shapefiles
    while read pass; do
	while read list; do
	    part=`echo $list | cut -d '_' -f 4`
	    job="ar_"$part
	    echo \#\!/bin/bash > $job
	    echo \#\PBS -lother=gdata1 >> $job
	    echo \#\PBS -l walltime=08:00:00 >> $job
	    echo \#\PBS -l mem=32GB >> $job
	    echo \#\PBS -l ncpus=1 >> $job
	    echo \#\PBS -l wd >> $job
	    echo \#\PBS -q express >> $job
	    echo source /g/data1/dg9/sll547/miniconda3/etc/profile.d/conda.sh >> $job
	    echo conda activate S1_Frames >> $job
	    echo ~/repo/gamma_insar/S1_FRAME_CREATION/create_S1_archive_dataframe.py $proj_dir/$config_file ~/repo/gamma_insar/S1_FRAME_CREATION/S1_burst_functions $pass $list >> $job
	    echo ~/repo/gamma_insar/S1_FRAME_CREATION/create_S1_archive_shapefiles.py $proj_dir/$config_file ~/repo/gamma_insar/S1_FRAME_CREATION/S1_burst_functions $pass $list >> $job
	    chmod +x $job
	    qsub $job 
	done < $archive_dir/$pass"_track_data"/$pass"_list"
    done < $burst_dir/pass_names

else
    echo "Option to create archive lists, dataframes and shapefiles from extracted metadata not selected."
fi
